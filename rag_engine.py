import os
import streamlit as st  # å¿…é¡»å¯¼å…¥ streamlit
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ================= é…ç½®åŒº =================
EMBEDDING_MODEL = "BAAI/bge-m3"
LLM_MODEL = "deepseek-ai/DeepSeek-V3"

class RAGPro:
    def __init__(self):
        # -------------------------------------------------------
        # ğŸ›¡ï¸ å®‰å…¨åŒºï¼šåœ¨å‡½æ•°å†…éƒ¨å®šä¹‰å˜é‡ï¼Œé˜²æ­¢ NameError
        # -------------------------------------------------------
        
        # 1. å®šä¹‰ Base URL (ç›´æ¥å†™æ­»åœ¨è¿™é‡Œï¼Œç»å¯¹ä¸ä¼šæ‰¾ä¸åˆ°)
        base_url = "https://api.siliconflow.cn/v1"

        # 2. è·å– API Key
        # ä¼˜å…ˆè¯»å– Streamlit Secretsï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨ç©ºå­—ç¬¦ä¸²å ä½
        if "OPENAI_API_KEY" in st.secrets:
            api_key = st.secrets["OPENAI_API_KEY"]
        elif "SILICON_API_KEY" in st.secrets:
            api_key = st.secrets["SILICON_API_KEY"]
        else:
            api_key = "key_not_found"

        # -------------------------------------------------------
        # ğŸ‘‡ åˆå§‹åŒ–æ¨¡å‹ ğŸ‘‡
        # -------------------------------------------------------
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=api_key,       # ä½¿ç”¨åˆšæ‰å®šä¹‰çš„å˜é‡
            openai_api_base=base_url,     # ä½¿ç”¨åˆšæ‰å®šä¹‰çš„å˜é‡
            check_embedding_ctx_length=False,
            chunk_size=50
        )
        
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            openai_api_key=api_key,       # ä½¿ç”¨åˆšæ‰å®šä¹‰çš„å˜é‡
            openai_api_base=base_url,     # ä½¿ç”¨åˆšæ‰å®šä¹‰çš„å˜é‡
            temperature=0.1
        )
        
        self.db_path = "./chroma_db_pro"
        self.vector_store = None

    def load_and_index(self, pdf_path):
        print(f"ğŸ“š æ­£åœ¨å¤„ç†æ–‡ä»¶: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(docs)

        # å°è¯•æ¸…ç†æ—§æ•°æ®åº“ (é˜²æ­¢æƒé™é”™è¯¯)
        if os.path.exists(self.db_path):
            try:
                import shutil
                shutil.rmtree(self.db_path)
            except:
                pass 
        
        self.vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.db_path
        )
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def query(self, question):
        if not self.vector_store:
            self.vector_store = Chroma(
                persist_directory=self.db_path, 
                embedding_function=self.embeddings
            )
        
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})
        relevant_docs = retriever.invoke(question)

        context_str = "\n\n".join([doc.page_content for doc in relevant_docs])
        source_pages = sorted(list(set([doc.metadata.get('page', 0) + 1 for doc in relevant_docs])))
        
        template = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ–‡æ¡£åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”é—®é¢˜ã€‚
        è§„åˆ™ï¼š1. å¿…é¡»å®Œå…¨åŸºäºå‚è€ƒèµ„æ–™å›ç­”ã€‚2. èµ„æ–™é‡Œæ²¡æœ‰å°±è¯´ä¸çŸ¥é“ã€‚
        ã€å‚è€ƒèµ„æ–™ã€‘ï¼š{context}
        ã€é—®é¢˜ã€‘ï¼š{question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        chain = (
            {"context": lambda x: context_str, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        answer = chain.invoke(question)

        return {
            "answer": answer,
            "sources": source_pages,
            "context": context_str
        }
