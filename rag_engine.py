import os
import time
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma  # å‡çº§ä½¿ç”¨æ–°ç‰ˆåº“
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ================= é…ç½®åŒº =================
# å¡«å…¥ä½ çš„ Key
if "OPENAI_API_KEY" in st.secrets:
    api_key = st.secrets["OPENAI_API_KEY"]
elif "SILICON_API_KEY" in st.secrets:
    api_key = st.secrets["SILICON_API_KEY"]
else:
    api_key = "local_test_key" # æœ¬åœ°æµ‹è¯•ç”¨
SILICON_BASE_URL = "https://api.siliconflow.cn/v1"

EMBEDDING_MODEL = "BAAI/bge-m3"
LLM_MODEL = "deepseek-ai/DeepSeek-V3"


class RAGPro:
    def __init__(self):
        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (è®°å¾—åŠ  chunk_size é˜²æ­¢æŠ¥é”™)
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=api_key,
            openai_api_base=base_url,
            check_embedding_ctx_length=False,
            chunk_size=50  # å…³é”®ä¿®æ­£
        )

        # 2. åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=0.1
        )

        self.db_path = "./chroma_db_pro"
        self.vector_store = None

    def load_and_index(self, pdf_path):
        """åŠ è½½PDF -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å…¥æ•°æ®åº“"""
        print(f"ğŸ“š æ­£åœ¨å¤„ç†æ–‡ä»¶: {pdf_path}")

        # åŠ è½½
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        print(f"ğŸ“„ å…±åŠ è½½ {len(docs)} é¡µ")

        # åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # åŠ å¤§ä¸€ç‚¹ï¼Œä¿è¯è¯­ä¹‰å®Œæ•´
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(docs)
        print(f"âœ‚ï¸ åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

        # å…¥åº“ (å¼ºåˆ¶åˆ·æ–°æ•°æ®åº“)
        if os.path.exists(self.db_path):
            try:
                import shutil
                shutil.rmtree(self.db_path) # åˆ é™¤æ—§åº“
            except:
                pass

        print("ğŸ’¾ æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼• (å¯èƒ½éœ€è¦å‡ åç§’)...")
        self.vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.db_path
        )
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def query(self, question):
        """æ ¸å¿ƒé—®ç­”é€»è¾‘ï¼šè¿”å›ç­”æ¡ˆ + æ¥æºé¡µç """
        if not self.vector_store:
            # å¦‚æœå†…å­˜é‡Œæ²¡æœ‰ï¼Œå°±å°è¯•ä»ç¡¬ç›˜åŠ è½½
            self.vector_store = Chroma(
                persist_directory=self.db_path,
                embedding_function=self.embeddings
            )

        # 1. æ£€ç´¢ (Retrieval)
        # k=3 è¡¨ç¤ºæ‰¾æœ€ç›¸å…³çš„3ä¸ªç‰‡æ®µ
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})
        relevant_docs = retriever.invoke(question)

        # 2. æ„å»ºä¸Šä¸‹æ–‡ (Context)
        # æˆ‘ä»¬ä¸ä»…è¦æ‹¼åˆæ–‡å­—ï¼Œè¿˜è¦æå–é¡µç 
        context_str = "\n\n".join([doc.page_content for doc in relevant_docs])

        # æå–æ¥æºä¿¡æ¯ (å»é‡)
        # metadata['page'] æ˜¯ä»0å¼€å§‹çš„ï¼Œæ‰€ä»¥è¦+1
        source_pages = sorted(list(set([doc.metadata.get('page', 0) + 1 for doc in relevant_docs])))

        # 3. Prompt
        template = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ–‡æ¡£åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”é—®é¢˜ã€‚

        è§„åˆ™ï¼š
        1. å¿…é¡»å®Œå…¨åŸºäºå‚è€ƒèµ„æ–™å›ç­”ã€‚
        2. å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰æåˆ°çš„ï¼Œè¯·ç›´æ¥è¯´â€œæ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹â€ã€‚
        3. ä¿æŒå›ç­”ç®€æ´æ˜äº†ã€‚

        ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
        {context}

        ã€é—®é¢˜ã€‘ï¼š
        {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        # 4. ç”Ÿæˆå›ç­”
        chain = (
                {"context": lambda x: context_str, "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
        )

        answer = chain.invoke(question)

        # 5. è¿”å›ç»“æ„åŒ–ç»“æœ (ç­”æ¡ˆ + æ¥æº)
        return {
            "answer": answer,
            "sources": source_pages,
            "context": context_str
        }


# ================= æµ‹è¯•ä»£ç  =================
if __name__ == "__main__":
    # ç¬¬ä¸€æ¬¡è¿è¡Œè¯·å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œæ¥æ„å»ºåº“
    # å‡è®¾ä½ æ”¾äº†ä¸€ä¸ª handbook.pdf åœ¨ data æ–‡ä»¶å¤¹
    bot = RAGPro()
    bot.load_and_index("./data/handbook.pdf")

    # æµ‹è¯•é—®ç­”
    q = "æ—·è¯¾ä¼šæœ‰ä»€ä¹ˆåæœ?"  # è¯·æ ¹æ®ä½ çš„ PDF å†…å®¹æé—®
    print(f"\nâ“ é—®é¢˜: {q}")

    try:
        result = bot.query(q)
        print(f"ğŸ¤– å›ç­”: {result['answer']}")
        print(f"ğŸ“– æ¥æºé¡µç : ç¬¬ {result['sources']} é¡µ")
    except Exception as e:

        print(f"âŒ æŠ¥é”™: {e}")


